#RNN/LSTM Overview

RNNs were created because of lack of functionality in traditional Neural Networks

Requirements:
- Not Learn from scratch everytime
- Preserve previous knowledge
- Infer dependancies between previous and current states

`Recurrent Neural Networks` allow persistance of information through various states. Essentially they are
networks with loops in them which provides information feedback. Hence the name `Recurrent`

So the output of the NN is fed back to the NN. Intuitively you can think of this as a list architecture i.e. a long chain of various NNs linked together.

`LSTM(Long Short Term Memory)` Networks are just a special kind of RNN.

Refer colah.github.io Understanding LSTM Networks


